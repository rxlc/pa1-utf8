**Another encoding of Unicode is UTF-32, which encodes all Unicode code points in 4 bytes. For things like ASCII, the leading 3 bytes are all 0's. What are some tradeoffs between UTF-32 and UTF-8?**
Since for every character in UTF-32 takes up 4 bytes, regardless of their code point, UTF-32 at first glance takes up a lot of memory especially when mostly ASCII values are stored. This makes UTF-8 much more efficient when characters with less code point values/bytes are used. On the other hand, UTF-32 allows for a quicker access to characters since the amount of possible bytes they take up are fixed.

**UTF-8 has a leading 10 on all the bytes past the first for multi-byte code points. This seems wasteful â€“ if the encoding for 3 bytes were instead 1110XXXX XXXXXXXX XXXXXXXX (where X can be any bit), that would fit 20 bits, which is over a million code points worth of space, removing the need for a 4-byte encoding. What are some tradeoffs or reasons the leading 10 might be useful? Can you think of anything that could go wrong with some programs if the encoding didn't include this restriction on multi-byte code points?**
If that restriction was not included, it could cause some potential issues where continuing bytes can't be identified in certain senarios where the bytes were cut off (Ex. invalid or corrupted utf-8s). This could also make looking for starting bytes are little more difficult, since there are some cases where the continuing byte can be similar to one of the starting prefixes, making the program more prone to errors.
